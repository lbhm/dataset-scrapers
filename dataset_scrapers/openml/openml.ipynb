{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from openml import datasets\n",
    "from openml.exceptions import OpenMLServerException\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(os.getenv(\"RAW_DATADIR\", \"../data\")) / \"openml\"\n",
    "base_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_catalog = datasets.list_datasets(output_format=\"dataframe\")\n",
    "# Remove datasets with server errors\n",
    "oml_catalog = oml_catalog[~oml_catalog.did.isin([4537, 4546, 4562, 40864, 41190, 41949])]\n",
    "# The following dataset profiles are too large for MongoDB (> 16MB)\n",
    "oml_catalog = oml_catalog[\n",
    "    ~oml_catalog.did.isin([41147, 42706, 42708, 44538, 44539, 44540, 44541, 44542])\n",
    "]\n",
    "\n",
    "oml_catalog.to_parquet(base_path / \"oml_catalog.pq\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = []\n",
    "errors = []\n",
    "\n",
    "for did in tqdm(oml_catalog.did):\n",
    "    try:\n",
    "        ds_list.append(datasets.get_dataset(did, download_qualities=True))\n",
    "    except OpenMLServerException as e:\n",
    "        # Error code for quality information not being available\n",
    "        if e.code == 362:\n",
    "            ds_list.append(datasets.get_dataset(did, download_qualities=False))\n",
    "        else:\n",
    "            errors.append((did, type(e), e.args))\n",
    "    except Exception as e:\n",
    "        errors.append((did, type(e), e.args))\n",
    "\n",
    "for did, e, args in errors:\n",
    "    print(f\"{did}: {e}\\n{args}\\n\")\n",
    "\n",
    "if len(errors) != 0:\n",
    "    print(errors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ds_list:\n",
    "    profile = {\n",
    "        \"dataset_id\": ds.dataset_id,\n",
    "        \"name\": ds.name,\n",
    "        \"version\": ds.version,\n",
    "        \"description\": ds.description,\n",
    "        \"creator\": ds.creator,\n",
    "        \"contributor\": ds.contributor,\n",
    "        \"collection_date\": ds.collection_date,\n",
    "        \"upload_date\": ds.upload_date,\n",
    "        \"language\": ds.language,\n",
    "        \"license\": ds.licence,\n",
    "        \"default_target_attribute\": ds.default_target_attribute,\n",
    "        \"row_id_attribute\": ds.row_id_attribute,\n",
    "        \"ignore_attribute\": ds.ignore_attribute,\n",
    "        \"tags\": ds.tag,\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"index\": v.index,\n",
    "                \"name\": v.name,\n",
    "                \"data_type\": v.data_type,\n",
    "                \"nominal_values\": v.nominal_values,\n",
    "                \"number_missing_values\": v.number_missing_values,\n",
    "            }\n",
    "            for k, v in ds.features.items()\n",
    "        ],\n",
    "        \"qualities\": ds.qualities,\n",
    "    }\n",
    "\n",
    "    collection_path = base_path / \"collection\"\n",
    "    collection_path.mkdir(exist_ok=True)\n",
    "    with (collection_path / f\"{ds.dataset_id}.json\").open(\"w\") as file:\n",
    "        json.dump(profile, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Parquet in Tabular Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_list = []\n",
    "f_list = []\n",
    "q_list = []\n",
    "t_list = []\n",
    "\n",
    "for ds in ds_list:\n",
    "    metadata = copy(vars(ds))\n",
    "\n",
    "    # List or dict attributes go to separate tables for normalization\n",
    "    features = metadata.pop(\"features\", {})\n",
    "    qualities = metadata.pop(\"qualities\", {})\n",
    "    tags = metadata.pop(\"tag\", [])\n",
    "    creators = metadata.pop(\"creator\", [])\n",
    "    contributors = metadata.pop(\"contributor\", [])\n",
    "    ignore_attributes = metadata.pop(\"ignore_attribute\", [])\n",
    "\n",
    "    if ignore_attributes:\n",
    "        if len(ignore_attributes) == 1:\n",
    "            ignore_attributes = ignore_attributes[0].split(\",\")\n",
    "    else:\n",
    "        ignore_attributes = []\n",
    "\n",
    "    for v in features.values():\n",
    "        f = copy(vars(v))\n",
    "        f[\"dataset_id\"] = ds.dataset_id\n",
    "        f[\"ignore\"] = False\n",
    "        if f[\"name\"] in ignore_attributes:\n",
    "            f[\"ignore\"] = True\n",
    "        f_list.append(f)\n",
    "\n",
    "    if qualities is not None:\n",
    "        for k, v in qualities.items():\n",
    "            q_list.append({\"dataset_id\": ds.dataset_id, \"metric\": k, \"value\": v})\n",
    "\n",
    "    if tags is not None:\n",
    "        t_list = [{\"dataset_id\": ds.dataset_id, \"tag\": t} for t in tags]\n",
    "\n",
    "    # The following attributes do not contain any data\n",
    "    _ = metadata.pop(\"update_comment\", None)\n",
    "    _ = metadata.pop(\"_dataset\", None)\n",
    "    _ = metadata.pop(\"data_pickle_file\", None)\n",
    "    _ = metadata.pop(\"data_feather_file\", None)\n",
    "    _ = metadata.pop(\"feather_attribute_file\", None)\n",
    "\n",
    "    # The following attributes always contain the same data\n",
    "    _ = metadata.pop(\"cache_format\", None)\n",
    "    _ = metadata.pop(\"format\", None)\n",
    "    _ = metadata.pop(\"visibility\", None)\n",
    "\n",
    "    md_list.append(metadata)\n",
    "\n",
    "dataset_df = pd.DataFrame(md_list)\n",
    "feature_df = pd.DataFrame(f_list)\n",
    "quality_df = pd.DataFrame(q_list)\n",
    "tags_df = pd.DataFrame(t_list)\n",
    "\n",
    "col = feature_df.pop(\"dataset_id\")\n",
    "feature_df.insert(0, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_parquet(base_path / \"datasets.pq\", index=False)\n",
    "feature_df.to_parquet(base_path / \"features.pq\", index=False)\n",
    "quality_df.to_parquet(base_path / \"metrics.pq\", index=False)\n",
    "tags_df.to_parquet(base_path / \"tags.pq\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "OpenML data types: [[\"nominal\", \"numeric\", \"string\", \"date\"]](https://github.com/openml/openml-python/blob/develop/openml/datasets/data_feature.py#L23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_catalog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cleansed = feature_df[\n",
    "    ~feature_df.name.str.contains(\"^oz[1-9][0-9]?[0-9]?$\")\n",
    "    & ~feature_df.name.str.contains(\"^V[0-9][0-9]?[0-9]?$\")\n",
    "    & ~feature_df.name.str.contains(\"^col_[0-9][0-9]?[0-9]?$\")\n",
    "    & ~feature_df.name.str.contains(\"AFFX-\")\n",
    "    & ~feature_df.name.str.contains(r\"Var\\d{0,7}\")\n",
    "    & ~feature_df.name.str.contains(r\"att_\\d{0,7}\")\n",
    "    & ~feature_df.name.str.contains(r\"^\\d+$\")\n",
    "    & ~feature_df.name.str.contains(r\"\\d{2,7}(?:_\\w*)?_at$\")\n",
    "].merge(\n",
    "    dataset_df[~dataset_df.name.str.startswith(\"QSAR-TID\")].dataset_id,\n",
    "    how=\"right\",\n",
    "    on=\"dataset_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = features_cleansed.groupby(\"name\")[\"dataset_id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count.sort_values(ascending=False).to_csv(\n",
    "    base_path / \"feature_count.csv\", header=[\"count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cleansed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
